SETUP
Setting Up and Running Open-Source LLaMA Models Locally Using Ollama
To run open-source LLaMA models on your local system, follow the steps below:
  1. Install Ollama on Your System
      1. Visit https://ollama.com/download/windows
      2. Download the .exe file appropriate for your system and run the installer.
      3. This will install the Ollama runtime and CLI on your local machine.
  2. To download or pull particular models
      1. Navigate to https://github.com/ollama/ollama
      2. Scroll to the model list table to explore available LLaMA-based models.
      3. Choose a model and run it using the command prompt.
          ollama run gemma3:1b
  3. Add this model in code
  4. Check ollama document for further usage
      https://github.com/ollama/ollama


DEPENDENCIES
pip install -r requirements.txt


TO RUN THE APPLICATION
streamlit run .\app.py
